{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GTHAarXju8wp"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "#from scipy.misc import imread\n",
    "import pickle\n",
    "import os\n",
    "from shutil import copyfile\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "#from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "#from tensorflow.keras.layers.normalization import BatchNormalization\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "#from keras.layers.normalization import BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "#from keras.layers.pooling import MaxPooling2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "#from tensorflow.keras.layers.merge import Concatenate\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "#from tensorflow.keras.layers.core import Lambda, Flatten, Dense\n",
    "from tensorflow.keras.layers import Lambda, Flatten, Dense\n",
    "#from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.utils import CustomObjectScope\n",
    "\n",
    "#from tensorflow.keras.engine.topology import Layer\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "#from sklearn.utils import shuffle\n",
    "\n",
    "import numpy.random as rng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cvL1GQ2lu8wu"
   },
   "source": [
    "### Please give below paths as per the paths in your system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Rtkj2JOu8ww"
   },
   "outputs": [],
   "source": [
    "base_folder = \"/Users/MacBookPro/AIandMLNYP/AIProject/SpeakerRecognition/TIMIT/\"\n",
    "data_folder = base_folder + \"/data\"\n",
    "predict_audio_folder = base_folder + \"/Audio/Prediction/\"\n",
    "pickle_path = '/Users/MacBookPro/AIandMLNYP/AIProject/SpeakerRecognition/data/'\n",
    "output_folder = base_folder + \"/npydata/\"\n",
    "model_path = base_folder + 'Model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R3e-lQobu8w0",
    "outputId": "ac06a545-b105-4c4d-c195-42fbf217ba26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFCC Frames (for 3 sec audio):     188\n"
     ]
    }
   ],
   "source": [
    "# Run the following code as it is\n",
    "\n",
    "# Each of our sample (16khz) lasts exactly from 3 - 5  seconds. We will truncate at 3 secs with 16000 * 3 samples.\n",
    "#\n",
    "mfcc_hop_length = 256\n",
    "mfcc_max_frames = int(16000 * 3 / mfcc_hop_length) + 1\n",
    "\n",
    "print (\"MFCC Frames (for 3 sec audio):     %d\" % (mfcc_max_frames))\n",
    "\n",
    "\n",
    "num_classes = 10\n",
    "max_samples = 16000 * 3  # 5 seconds\n",
    "max_mfcc_features = 40\n",
    "\n",
    "# Scale the values to be between \n",
    "def scale(arr):\n",
    "    #arr = arr - arr.mean()\n",
    "    safe_max = np.abs(arr).max()\n",
    "    if safe_max == 0:\n",
    "        safe_max = 1\n",
    "    arr = arr / safe_max\n",
    "    return arr\n",
    "\n",
    "\n",
    "# Load a file and convert its audio signal into a series of MFCC\n",
    "# This will return a 2D numpy array.\n",
    "#\n",
    "def convert_mfcc(file_name):\n",
    "    signal, sample_rate = librosa.load(file_name) \n",
    "    signal = librosa.util.normalize(signal)\n",
    "    signal_trimmed, index = librosa.effects.trim(signal, top_db=60)\n",
    "    signal_trimmed = librosa.util.fix_length(signal_trimmed, max_samples)\n",
    "    \n",
    "    feature = (librosa.feature.mfcc(y=signal_trimmed, sr=sample_rate, n_mfcc=max_mfcc_features).T)\n",
    "    #print (feature.shape)\n",
    "    if (feature.shape[0] > mfcc_max_frames):\n",
    "        feature = feature[0:mfcc_max_frames, :]\n",
    "    if (feature.shape[0] < mfcc_max_frames):\n",
    "        feature = np.pad(feature, pad_width=((0, mfcc_max_frames - feature.shape[0]), (0,0)), mode='constant')\n",
    "    \n",
    "    # This removes the average component from the MFCC as it may not be meaningful.\n",
    "    #\n",
    "    feature[:,0] = 0\n",
    "        \n",
    "    feature = scale(feature)\n",
    "    #print(feature)\n",
    "    return feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SlC4KoMRu8w4"
   },
   "outputs": [],
   "source": [
    "def convertAudio(path,n = 0):\n",
    "    '''\n",
    "    path => Path of train directory or test directory\n",
    "    '''\n",
    "    X_class = []\n",
    "    y_class = []\n",
    "    # convert and load the mfcc features for all audio files for each speaker\n",
    "    file_path = path + \"Audio/Prediction/\"\n",
    "    for speaker in os.listdir(file_path):\n",
    "        print(\"loading speaker: \" + speaker)\n",
    "        speaker_path = os.path.join(file_path,speaker)\n",
    "        for filename in os.listdir(speaker_path):\n",
    "            audio_file_path = os.path.join(speaker_path, filename)\n",
    "            # convert the audio file to mfcc\n",
    "            mfcc = convert_mfcc(audio_file_path)\n",
    "                #print(image.shape)\n",
    "            X_class.append(mfcc)\n",
    "            y_class.append(speaker)\n",
    "    y_class = np.stack(y_class)\n",
    "    X_class = np.stack(X_class)\n",
    "    print(y_class)\n",
    "    return  X_class, y_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LrGiAJfsu8w7"
   },
   "source": [
    "### Loading the train audio files into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GM1LRXAGu8w9",
    "outputId": "65fa096f-f099-41f6-e164-8af3e2f065fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading speaker: FDAW0\n",
      "loading speaker: MTPF0\n",
      "loading speaker: MGRL0\n",
      "loading speaker: MTJS0\n",
      "loading speaker: MMRP0\n",
      "loading speaker: FSJK1\n",
      "loading speaker: MDPK0\n",
      "loading speaker: Sekhar\n",
      "loading speaker: MKLW0\n",
      "loading speaker: MRCG0\n",
      "loading speaker: MJEB1\n",
      "loading speaker: Joy\n",
      "loading speaker: Sindhu\n",
      "['FDAW0' 'FDAW0' 'FDAW0' 'FDAW0' 'FDAW0' 'FDAW0' 'FDAW0' 'FDAW0' 'FDAW0'\n",
      " 'FDAW0' 'MTPF0' 'MTPF0' 'MTPF0' 'MTPF0' 'MTPF0' 'MTPF0' 'MTPF0' 'MTPF0'\n",
      " 'MTPF0' 'MTPF0' 'MGRL0' 'MGRL0' 'MGRL0' 'MGRL0' 'MGRL0' 'MGRL0' 'MGRL0'\n",
      " 'MGRL0' 'MGRL0' 'MGRL0' 'MTJS0' 'MTJS0' 'MTJS0' 'MTJS0' 'MTJS0' 'MTJS0'\n",
      " 'MTJS0' 'MTJS0' 'MTJS0' 'MTJS0' 'MMRP0' 'MMRP0' 'MMRP0' 'MMRP0' 'MMRP0'\n",
      " 'MMRP0' 'MMRP0' 'MMRP0' 'MMRP0' 'MMRP0' 'FSJK1' 'FSJK1' 'FSJK1' 'FSJK1'\n",
      " 'FSJK1' 'FSJK1' 'FSJK1' 'FSJK1' 'FSJK1' 'FSJK1' 'MDPK0' 'MDPK0' 'MDPK0'\n",
      " 'MDPK0' 'MDPK0' 'MDPK0' 'MDPK0' 'MDPK0' 'MDPK0' 'MDPK0' 'Sekhar' 'Sekhar'\n",
      " 'Sekhar' 'Sekhar' 'Sekhar' 'Sekhar' 'Sekhar' 'Sekhar' 'Sekhar' 'MKLW0'\n",
      " 'MKLW0' 'MKLW0' 'MKLW0' 'MKLW0' 'MKLW0' 'MKLW0' 'MKLW0' 'MKLW0' 'MKLW0'\n",
      " 'MRCG0' 'MRCG0' 'MRCG0' 'MRCG0' 'MRCG0' 'MRCG0' 'MRCG0' 'MRCG0' 'MRCG0'\n",
      " 'MJEB1' 'MJEB1' 'MJEB1' 'MJEB1' 'MJEB1' 'MJEB1' 'MJEB1' 'MJEB1' 'MJEB1'\n",
      " 'Joy' 'Joy' 'Joy' 'Joy' 'Joy' 'Joy' 'Joy' 'Joy' 'Joy' 'Joy' 'Sindhu'\n",
      " 'Sindhu' 'Sindhu' 'Sindhu' 'Sindhu' 'Sindhu' 'Sindhu' 'Sindhu' 'Sindhu']\n",
      "(126, 188, 40)\n"
     ]
    }
   ],
   "source": [
    "X, y =convertAudio(base_folder)\n",
    "print(X.shape)\n",
    "#print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HvNV1bzOu8xA"
   },
   "source": [
    "### Saving the baseline tensors on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Nh0zQyRu8xB"
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(pickle_path,\"employeeaudio.pickle\"), \"wb\") as f:\n",
    "    pickle.dump((X,y),f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8VISF8Y5u8xE"
   },
   "source": [
    "### Loading the validation audio  into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pAd0Y9EAu8xF",
    "outputId": "99068337-609f-4c4e-86c2-62ed665a4b76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(126, 188, 40)\n",
      "(126,)\n"
     ]
    }
   ],
   "source": [
    "###generate pairs and targets\n",
    "with open(os.path.join(pickle_path, \"employeeaudio.pickle\"), \"rb\") as f:\n",
    "    (X, y) = pickle.load(f)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_UmEiVGEu8xI"
   },
   "outputs": [],
   "source": [
    "input_file_path = \"/Users/MacBookPro/AIandMLNYP/AIProject/SpeakerRecognition/TIMIT/Audio/Input/\"\n",
    "#input_file_path = \"/Users/MacBookPro/AIandMLNYP/AIProject/SpeakerRecognition/TIMIT/Audio/Prediction/Joy/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vRSwmVoAu8xO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ztKVhWQGu8xR"
   },
   "outputs": [],
   "source": [
    "def initialize_weights(shape, dtype=None):\n",
    "    return np.random.normal(loc = 0.0, scale = 1e-2, size = shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZOqUiCXHu8xU"
   },
   "outputs": [],
   "source": [
    "def initialize_bias(shape, dtype=None):\n",
    "    return np.random.normal(loc = 0.5, scale = 1e-2, size = shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f7kX3rD_u8xW"
   },
   "outputs": [],
   "source": [
    "def get_base_conv_encoder(input_shape):\n",
    "     # Convolutional Neural Network\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (3,3), activation='relu', input_shape=input_shape,\n",
    "                   #kernel_initializer=initialize_weights, kernel_regularizer=l2(2e-4)))\n",
    "                   kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None), kernel_regularizer=l2(2e-4)))  \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(128, (3,3), activation='relu',\n",
    "                     kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None),\n",
    "                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(516, activation='sigmoid',\n",
    "                   kernel_regularizer=l2(1e-3),\n",
    "                   kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None),bias_initializer=initialize_bias))\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8fbt2F8iu8xZ"
   },
   "outputs": [],
   "source": [
    "def build_final_model(input_shape,  distance_metric='uniform_euclidean'):\n",
    "    \n",
    "    assert distance_metric in ('uniform_euclidean', \n",
    "                                'weighted_l1',\n",
    "                                'cosine_distance')\n",
    "    left_input = Input(input_shape)\n",
    "    right_input = Input(input_shape)\n",
    "    model = get_base_conv_encoder(input_shape)\n",
    "    encoded_l = model(left_input)\n",
    "    encoded_r = model(right_input)\n",
    "    \n",
    "    \n",
    "    if distance_metric == 'weighted_l1':\n",
    "        print(\"using Weighted_l1\")\n",
    "        L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "        L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "        prediction = Dense(1,activation='sigmoid',bias_initializer=initialize_bias)(L1_distance)\n",
    "      \n",
    "    if distance_metric == 'uniform_euclidean':\n",
    "        print(\"inside euclidian\")\n",
    "        L1_layer = Lambda(lambda tensors:K.sqrt(K.sum(K.square(K.abs(tensors[0] - tensors[1])),axis=-1, keepdims=True)))\n",
    "        L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "        prediction = Dense(1,activation='sigmoid',bias_initializer=initialize_bias)(L1_distance)\n",
    "\n",
    "   \n",
    "    if distance_metric == 'cosine_distance':\n",
    "        print(\"using cosine similarity\")\n",
    "        L1_layer = Lambda(cosine_similarity, output_shape=cos_dist_output_shape)\n",
    "        L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "        prediction = Dense(1,activation='sigmoid',bias_initializer=initialize_bias)(L1_distance)\n",
    "    \n",
    "    \n",
    "    # Connect the inputs with the outputs\n",
    "    siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "    # return the model\n",
    "    return siamese_net  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z39SQ7n0u8xb",
    "outputId": "d9d5b464-c98d-4a08-e699-4c2784a4c3b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside euclidian\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 188, 40, 1)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 188, 40, 1)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, 516)          23852804    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 1)            0           sequential[1][0]                 \n",
      "                                                                 sequential[2][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            2           lambda[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 23,852,806\n",
      "Trainable params: 23,852,550\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_final_model((188, 40, 1),'uniform_euclidean')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hXDHph0Ou8xg"
   },
   "outputs": [],
   "source": [
    "#model.load_weights(os.path.join(model_path, \"seg_weights.best.hdf5\"))\n",
    "model.load_weights(os.path.join(model_path, \"seg_uniform_euclidean_weights.best.hdf5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ii5XhtQ9u8xj"
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(lr = 0.00006)\n",
    "model.compile(loss=\"binary_crossentropy\",metrics=['accuracy'], optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mrh4Rpgou8xl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3NJVpq70u8xo"
   },
   "outputs": [],
   "source": [
    "def predict_speaker(inputaudiofile):\n",
    "\n",
    "    \n",
    "    ### extract audio mfccs for the inputaudiofile.\n",
    "    audio_file_path = os.path.join(input_file_path, inputaudiofile)\n",
    "    mfcc = convert_mfcc(audio_file_path)\n",
    "        \n",
    "    input_l = []\n",
    "    input_r = []\n",
    "    s,w,h = X.shape\n",
    "    input_l.append(mfcc.reshape(w,h,1))\n",
    "    y_pred = np.zeros(shape=(s))\n",
    "    \n",
    "    ###loop through the audio mfccs of the baseline audio files.\n",
    "     \n",
    "    for  i in range(X.shape[0]):\n",
    "        w,h = mfcc.shape\n",
    "        pairs=[np.zeros((1 , w, h,1)) for i in range(2)]\n",
    "        pairs[0][0,:,:,:] = input_l[0]\n",
    "        pairs[1][0,:,:,:] = X[i].reshape(w,h,1)\n",
    "        y_pred[i] = (model.predict(pairs).ravel())[0]\n",
    "        \n",
    "    return y_pred\n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XJQtty3ru8xq"
   },
   "outputs": [],
   "source": [
    "y_pred = predict_speaker('MJEB1_SI837.WAV.wav')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U8AjbakEu8xs",
    "outputId": "6306a4e9-7ad5-4c72-9571-0a1bfc9d7870"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MJEB1\n"
     ]
    }
   ],
   "source": [
    "prediction = y[np.argmax(y_pred)]\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QoGYILtzu8xx",
    "outputId": "de8830f1-9580-430a-bccd-74a36d385289"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRCG0\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_speaker('MRC0_SX438.WAV.wav')\n",
    "prediction = y[np.argmax(y_pred)]\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sqsF1JdOu8x2",
    "outputId": "e9f6bff6-03fb-4f87-c41a-6f97e815cb84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joy\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_speaker('sx52-Sindhu.wav')\n",
    "prediction = y[np.argmax(y_pred)]\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TI34XJMSu8x5",
    "outputId": "03d37a5e-3832-42ec-a697-6b60cab116c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joy\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_speaker('sx30.wav')\n",
    "prediction = y[np.argmax(y_pred)]\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vy0f_eUOu8x7",
    "outputId": "963298c1-0033-497e-9738-04dd0dc862ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sekhar\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_speaker('Sek10.wav')\n",
    "#print(y_pred)\n",
    "prediction = y[np.argmax(y_pred)]\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9U0mK3O4u8x_",
    "outputId": "9a9d75f0-cafa-4838-db07-f0a0ce41c3bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Sekhar', 0.5519243478775024), ('Sekhar', 0.5204748511314392), ('Sekhar', 0.44365188479423523), ('Sekhar', 0.4145190119743347), ('MKLW0', 0.3665523827075958), ('MMRP0', 0.35389444231987), ('MKLW0', 0.34238049387931824), ('MKLW0', 0.3412140905857086), ('MKLW0', 0.31892716884613037), ('MTPF0', 0.3159143626689911), ('MKLW0', 0.3143109679222107), ('MKLW0', 0.311082124710083), ('MKLW0', 0.3028748631477356), ('MDPK0', 0.29807695746421814), ('MTPF0', 0.291708379983902), ('MKLW0', 0.28600847721099854), ('MGRL0', 0.2844463586807251), ('MDPK0', 0.2832947075366974), ('MTPF0', 0.28032538294792175), ('MKLW0', 0.2737398147583008), ('MDPK0', 0.2640925347805023), ('Sekhar', 0.2603056728839874), ('MMRP0', 0.2594367265701294), ('MTPF0', 0.2579585611820221), ('MTPF0', 0.24718894064426422), ('MMRP0', 0.24402746558189392), ('MDPK0', 0.2430187314748764), ('MTPF0', 0.24067723751068115), ('Sekhar', 0.23950383067131042), ('MKLW0', 0.2351701557636261), ('MGRL0', 0.23139213025569916), ('MMRP0', 0.23037780821323395), ('MTPF0', 0.22949577867984772), ('MTPF0', 0.22599607706069946), ('MGRL0', 0.22414670884609222), ('MMRP0', 0.22056768834590912), ('MTPF0', 0.2185152918100357), ('MGRL0', 0.2183617651462555), ('MMRP0', 0.21438013017177582), ('MGRL0', 0.21300527453422546), ('MMRP0', 0.2112192064523697), ('MGRL0', 0.21078722178936005), ('MTPF0', 0.2044072300195694), ('Sekhar', 0.20422857999801636), ('MDPK0', 0.20302727818489075), ('MGRL0', 0.19802740216255188), ('MMRP0', 0.19205878674983978), ('MGRL0', 0.18987275660037994), ('MMRP0', 0.18919284641742706), ('MGRL0', 0.18836326897144318), ('Sekhar', 0.1826314926147461), ('MDPK0', 0.17937400937080383), ('MMRP0', 0.1727696806192398), ('MDPK0', 0.17247320711612701), ('Sekhar', 0.17234008014202118), ('MDPK0', 0.16176962852478027), ('MDPK0', 0.15603294968605042), ('MRCG0', 0.15101072192192078), ('MDPK0', 0.14360640943050385), ('MJEB1', 0.1413327157497406), ('MJEB1', 0.13114021718502045), ('MGRL0', 0.12874622642993927), ('MJEB1', 0.12219712883234024), ('MJEB1', 0.1166907399892807), ('MTJS0', 0.10294654965400696), ('MJEB1', 0.09845352917909622), ('MTJS0', 0.09647747874259949), ('MJEB1', 0.09622035175561905), ('MJEB1', 0.09199842065572739), ('MTJS0', 0.08982598036527634), ('MTJS0', 0.08967504650354385), ('MTJS0', 0.08856553584337234), ('MTJS0', 0.08774722367525101), ('MJEB1', 0.08688226342201233), ('MJEB1', 0.0867510661482811), ('Sindhu', 0.08401010930538177), ('MTJS0', 0.08094727993011475), ('MTJS0', 0.0763620138168335), ('MTJS0', 0.07359156012535095), ('MTJS0', 0.07120262831449509), ('Sindhu', 0.06694608181715012), ('MRCG0', 0.06376588344573975), ('Sindhu', 0.06162543594837189), ('Sindhu', 0.06068972125649452), ('MRCG0', 0.05535348504781723), ('MRCG0', 0.05372247099876404), ('MRCG0', 0.046617742627859116), ('Sindhu', 0.045818429440259933), ('Sindhu', 0.04297858849167824), ('Sindhu', 0.04045667126774788), ('Sindhu', 0.03775139898061752), ('MRCG0', 0.03347822278738022), ('Sindhu', 0.03343496844172478), ('MRCG0', 0.03275848180055618), ('Joy', 0.031221505254507065), ('MRCG0', 0.028773803263902664), ('Joy', 0.02847733348608017), ('MRCG0', 0.027636414393782616), ('Joy', 0.024760974571108818), ('Joy', 0.022949565201997757), ('Joy', 0.021719615906476974), ('Joy', 0.021364489570260048), ('Joy', 0.021225132048130035), ('FSJK1', 0.020865172147750854), ('Joy', 0.02003972791135311), ('Joy', 0.019899895414710045), ('FSJK1', 0.019778648391366005), ('FSJK1', 0.018567346036434174), ('FSJK1', 0.01838495023548603), ('Joy', 0.018256576731801033), ('FSJK1', 0.017990294843912125), ('FDAW0', 0.017886903136968613), ('FSJK1', 0.01780521497130394), ('FSJK1', 0.017758043482899666), ('FDAW0', 0.01773032732307911), ('FDAW0', 0.01769847422838211), ('FSJK1', 0.017655905336141586), ('FDAW0', 0.017653580754995346), ('FDAW0', 0.017637191340327263), ('FDAW0', 0.017626957967877388), ('FDAW0', 0.01760350726544857), ('FDAW0', 0.017588727176189423), ('FDAW0', 0.01757984049618244), ('FDAW0', 0.01757221482694149), ('FSJK1', 0.017566150054335594), ('FSJK1', 0.017549198120832443)]\n"
     ]
    }
   ],
   "source": [
    "probs = list(zip(y, y_pred))\n",
    "sprobs = sorted(probs, key=lambda tup: tup[1], reverse = True)\n",
    "print(sprobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MJwR9soVu8yC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tmgJTconu8yE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "SpeakerRecognitionPredictionUniform.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
