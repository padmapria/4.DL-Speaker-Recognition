{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"4_SpeakerRecognition_Prediction_WeightedL1.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"nXZreQAM1xkb","colab_type":"text"},"source":["# ITI110 Project - Speaker Recognition\n","\n","Team 9\n","\n","## Part 3 - Predictions"]},{"cell_type":"code","metadata":{"id":"kY11kwyCsDWx","colab_type":"code","colab":{}},"source":["import sys\n","import numpy as np\n","import pandas as pd\n","import librosa\n","import pickle\n","import os\n","from shutil import copyfile\n","import matplotlib.pyplot as plt\n","import imageio\n","%matplotlib inline\n","\n","import cv2\n","import time\n","\n","import multiprocessing\n","\n","import tensorflow as tf\n","import tensorflow.keras\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n","from tensorflow.keras.models import Model\n","\n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras.layers import MaxPooling2D\n","from tensorflow.keras.layers import Concatenate\n","from tensorflow.keras.layers import Lambda, Flatten, Dense\n","from tensorflow.keras.initializers import glorot_uniform\n","from tensorflow.keras.utils import CustomObjectScope\n","\n","from tensorflow.keras.layers import Layer\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras import backend as K\n","from tensorflow.keras import initializers\n","\n","import numpy.random as rng"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ImL8hGGgsDXH","colab_type":"text"},"source":["### Folder Paths"]},{"cell_type":"code","metadata":{"id":"PmZvxhDksDXJ","colab_type":"code","colab":{}},"source":["base_folder = \"/Users/MacBookPro/AIandMLNYP/AIProject/SpeakerRecognition/TIMIT/\"\n","data_folder = base_folder + \"/data\"\n","predict_audio_folder = base_folder + \"/Audio/Prediction/\"\n","pickle_path = '/Users/MacBookPro/AIandMLNYP/AIProject/SpeakerRecognition/data/'\n","output_folder = base_folder + \"/npydata/\"\n","model_path = base_folder + 'Model'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K5a8-oTEsDX2","colab_type":"code","colab":{}},"source":["input_file_path = \"/Users/MacBookPro/AIandMLNYP/AIProject/SpeakerRecognition/TIMIT/Audio/Input/\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LQMpCLA8LIwH","colab_type":"text"},"source":["### Converting to MFCC"]},{"cell_type":"code","metadata":{"id":"jw69NcKEsDXQ","colab_type":"code","outputId":"21965e8e-b70f-458e-93a0-7d830fbf9fb5","colab":{}},"source":["# Each of our sample (16khz) lasts exactly from 3 - 5  seconds. We will truncate at 3 secs with 16000 * 3 samples.\n","#\n","mfcc_hop_length = 256\n","mfcc_max_frames = int(16000 * 3 / mfcc_hop_length) + 1\n","\n","print (\"MFCC Frames (for 3 sec audio):     %d\" % (mfcc_max_frames))\n","\n","\n","num_classes = 10\n","max_samples = 16000 * 3  # 5 seconds\n","max_mfcc_features = 40\n","\n","# Scale the values to be between \n","def scale(arr):\n","    #arr = arr - arr.mean()\n","    safe_max = np.abs(arr).max()\n","    if safe_max == 0:\n","        safe_max = 1\n","    arr = arr / safe_max\n","    return arr\n","\n","\n","# Load a file and convert its audio signal into a series of MFCC\n","# This will return a 2D numpy array.\n","#\n","def convert_mfcc(file_name):\n","    signal, sample_rate = librosa.load(file_name) \n","    signal = librosa.util.normalize(signal)\n","    signal_trimmed, index = librosa.effects.trim(signal, top_db=60)\n","    signal_trimmed = librosa.util.fix_length(signal_trimmed, max_samples)\n","    \n","    feature = (librosa.feature.mfcc(y=signal_trimmed, sr=sample_rate, n_mfcc=max_mfcc_features).T)\n","\n","    if (feature.shape[0] > mfcc_max_frames):\n","        feature = feature[0:mfcc_max_frames, :]\n","    if (feature.shape[0] < mfcc_max_frames):\n","        feature = np.pad(feature, pad_width=((0, mfcc_max_frames - feature.shape[0]), (0,0)), mode='constant')\n","    \n","    # This removes the average component from the MFCC as it may not be meaningful.\n","    #\n","    feature[:,0] = 0\n","        \n","    feature = scale(feature)\n","\n","    return feature\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["MFCC Frames (for 3 sec audio):     188\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mzrEb0XFsDXV","colab_type":"code","colab":{}},"source":["def convertAudio(path,n = 0):\n","    '''\n","    path => Path of train directory or test directory\n","    '''\n","    X_class = []\n","    y_class = []\n","    # convert and load the mfcc features for all audio files for each speaker\n","    file_path = path + \"Audio/Prediction/\"\n","    for speaker in os.listdir(file_path):\n","        print(\"loading speaker: \" + speaker)\n","        speaker_path = os.path.join(file_path,speaker)\n","        for filename in os.listdir(speaker_path):\n","            audio_file_path = os.path.join(speaker_path, filename)\n","            # convert the audio file to mfcc\n","            mfcc = convert_mfcc(audio_file_path)\n","                #print(image.shape)\n","            X_class.append(mfcc)\n","            y_class.append(speaker)\n","    y_class = np.vstack(y_class)\n","    X_class = np.stack(X_class)\n","    print(y_class)\n","    return  X_class, y_class"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f8i341jNMofm","colab_type":"text"},"source":["# Tensor Operations"]},{"cell_type":"markdown","metadata":{"id":"uSf4Iwx0sDXd","colab_type":"text"},"source":["### Loading new speakers' audio files into tensors"]},{"cell_type":"code","metadata":{"id":"6RaiXOXzsDXf","colab_type":"code","outputId":"211fe855-7617-4a4e-9688-a421a99e365a","colab":{}},"source":["X, y =convertAudio(base_folder)\n","print(X.shape)\n","#print(c)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["loading speaker: FDAW0\n","loading speaker: MTPF0\n","loading speaker: MGRL0\n","loading speaker: MTJS0\n","loading speaker: MMRP0\n","loading speaker: FSJK1\n","loading speaker: MDPK0\n","loading speaker: Sekhar\n","loading speaker: MKLW0\n","loading speaker: MRCG0\n","loading speaker: MJEB1\n","loading speaker: Joy\n","loading speaker: Sindhu\n","[['FDAW0']\n"," ['FDAW0']\n"," ['FDAW0']\n"," ['FDAW0']\n"," ['FDAW0']\n"," ['FDAW0']\n"," ['FDAW0']\n"," ['FDAW0']\n"," ['FDAW0']\n"," ['FDAW0']\n"," ['MTPF0']\n"," ['MTPF0']\n"," ['MTPF0']\n"," ['MTPF0']\n"," ['MTPF0']\n"," ['MTPF0']\n"," ['MTPF0']\n"," ['MTPF0']\n"," ['MTPF0']\n"," ['MTPF0']\n"," ['MGRL0']\n"," ['MGRL0']\n"," ['MGRL0']\n"," ['MGRL0']\n"," ['MGRL0']\n"," ['MGRL0']\n"," ['MGRL0']\n"," ['MGRL0']\n"," ['MGRL0']\n"," ['MGRL0']\n"," ['MTJS0']\n"," ['MTJS0']\n"," ['MTJS0']\n"," ['MTJS0']\n"," ['MTJS0']\n"," ['MTJS0']\n"," ['MTJS0']\n"," ['MTJS0']\n"," ['MTJS0']\n"," ['MTJS0']\n"," ['MMRP0']\n"," ['MMRP0']\n"," ['MMRP0']\n"," ['MMRP0']\n"," ['MMRP0']\n"," ['MMRP0']\n"," ['MMRP0']\n"," ['MMRP0']\n"," ['MMRP0']\n"," ['MMRP0']\n"," ['FSJK1']\n"," ['FSJK1']\n"," ['FSJK1']\n"," ['FSJK1']\n"," ['FSJK1']\n"," ['FSJK1']\n"," ['FSJK1']\n"," ['FSJK1']\n"," ['FSJK1']\n"," ['FSJK1']\n"," ['MDPK0']\n"," ['MDPK0']\n"," ['MDPK0']\n"," ['MDPK0']\n"," ['MDPK0']\n"," ['MDPK0']\n"," ['MDPK0']\n"," ['MDPK0']\n"," ['MDPK0']\n"," ['MDPK0']\n"," ['Sekhar']\n"," ['Sekhar']\n"," ['Sekhar']\n"," ['Sekhar']\n"," ['Sekhar']\n"," ['Sekhar']\n"," ['Sekhar']\n"," ['Sekhar']\n"," ['Sekhar']\n"," ['Sekhar']\n"," ['MKLW0']\n"," ['MKLW0']\n"," ['MKLW0']\n"," ['MKLW0']\n"," ['MKLW0']\n"," ['MKLW0']\n"," ['MKLW0']\n"," ['MKLW0']\n"," ['MKLW0']\n"," ['MKLW0']\n"," ['MRCG0']\n"," ['MRCG0']\n"," ['MRCG0']\n"," ['MRCG0']\n"," ['MRCG0']\n"," ['MRCG0']\n"," ['MRCG0']\n"," ['MRCG0']\n"," ['MRCG0']\n"," ['MRCG0']\n"," ['MJEB1']\n"," ['MJEB1']\n"," ['MJEB1']\n"," ['MJEB1']\n"," ['MJEB1']\n"," ['MJEB1']\n"," ['MJEB1']\n"," ['MJEB1']\n"," ['MJEB1']\n"," ['MJEB1']\n"," ['Joy']\n"," ['Joy']\n"," ['Joy']\n"," ['Joy']\n"," ['Joy']\n"," ['Joy']\n"," ['Joy']\n"," ['Joy']\n"," ['Joy']\n"," ['Joy']\n"," ['Joy']\n"," ['Sindhu']\n"," ['Sindhu']\n"," ['Sindhu']\n"," ['Sindhu']\n"," ['Sindhu']\n"," ['Sindhu']\n"," ['Sindhu']\n"," ['Sindhu']\n"," ['Sindhu']\n"," ['Sindhu']]\n","(131, 188, 40)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"46luVr3fsDXm","colab_type":"text"},"source":["### Saving the baseline tensors on disk"]},{"cell_type":"code","metadata":{"id":"5zOXf-pvsDXo","colab_type":"code","colab":{}},"source":["with open(os.path.join(pickle_path,\"employeeaudio.pickle\"), \"wb\") as f:\n","    pickle.dump((X,y),f)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uT0gEv2WsDXv","colab_type":"text"},"source":["### Loading the validation audio  into tensors"]},{"cell_type":"code","metadata":{"id":"zhwxlHUpsDXx","colab_type":"code","outputId":"fb567073-c7a4-4024-aa9f-d11ed72387f5","colab":{}},"source":["# generate pairs and targets\n","with open(os.path.join(pickle_path, \"employeeaudio.pickle\"), \"rb\") as f:\n","    (X, y) = pickle.load(f)\n","\n","print(X.shape)\n","print(y.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(131, 188, 40)\n","(131, 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bBEd_y73L7aX","colab_type":"text"},"source":["# Rebuild Best Model"]},{"cell_type":"markdown","metadata":{"id":"-v4w7cWIMTdS","colab_type":"text"},"source":["### Pull-in best model, along with Initializers and Optimizers"]},{"cell_type":"code","metadata":{"id":"KEVlMX3IsDYF","colab_type":"code","colab":{}},"source":["def initialize_weights(shape, dtype=None):\n","    return np.random.normal(loc = 0.0, scale = 1e-2, size = shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"n5OAOwg7sDYK","colab_type":"code","colab":{}},"source":["def initialize_bias(shape, dtype=None):\n","    return np.random.normal(loc = 0.5, scale = 1e-2, size = shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"N5pe9JzvsDYO","colab_type":"code","colab":{}},"source":["def get_base_conv_encoder(input_shape):\n","    # Convolutional Neural Network\n","    model = Sequential()\n","    model.add(Conv2D(64, (3,3), activation='relu', input_shape=input_shape,\n","                   kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None), kernel_regularizer=l2(2e-4)))  \n","    model.add(MaxPooling2D(pool_size=(2, 2)))\n","    model.add(Conv2D(128, (3,3), activation='relu',\n","                     kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None),\n","                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n","    model.add(BatchNormalization())\n","    model.add(MaxPooling2D(pool_size=(2, 2)))\n","    model.add(Flatten())\n","    model.add(Dense(516, activation='sigmoid',\n","                   kernel_regularizer=l2(1e-3),\n","                   kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None),bias_initializer=initialize_bias))\n","    return model\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dEQPRN-lsDYS","colab_type":"code","colab":{}},"source":["def build_final_model(input_shape,  distance_metric='uniform_euclidean'):\n","    \n","    assert distance_metric in ('uniform_euclidean', \n","                                'weighted_l1',\n","                                'cosine_distance')\n","    left_input = Input(input_shape)\n","    right_input = Input(input_shape)\n","    model = get_base_conv_encoder(input_shape)\n","    encoded_l = model(left_input)\n","    encoded_r = model(right_input)\n","    \n","    if distance_metric == 'weighted_l1':\n","        print(\"using Weighted_l1\")\n","        L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n","        L1_distance = L1_layer([encoded_l, encoded_r])\n","        prediction = Dense(1,activation='sigmoid',bias_initializer=initialize_bias)(L1_distance)\n","      \n","    if distance_metric == 'uniform_euclidean':\n","        print(\"inside euclidian\")\n","        L1_layer = Lambda(lambda tensors:K.sqrt(K.sum(K.square(K.abs(tensors[0] - tensors[1])),axis=-1, keepdims=True)))\n","        L1_distance = L1_layer([encoded_l, encoded_r])\n","        prediction = Dense(1,activation='sigmoid',bias_initializer=initialize_bias)(L1_distance)\n","\n","   \n","    if distance_metric == 'cosine_distance':\n","        print(\"using cosine similarity\")\n","        L1_layer = Lambda(cosine_similarity, output_shape=cos_dist_output_shape)\n","        L1_distance = L1_layer([encoded_l, encoded_r])\n","        prediction = Dense(1,activation='sigmoid',bias_initializer=initialize_bias)(L1_distance)\n","    \n","    \n","    # Connect the inputs with the outputs\n","    siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n","    # return the model\n","    return siamese_net  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FTRM_SbvsDYW","colab_type":"code","outputId":"f9759698-ae30-44d0-b41b-9d21a3039746","colab":{}},"source":["model = build_final_model((188, 40, 1),'weighted_l1')\n","model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["using Weighted_l1\n","Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, 188, 40, 1)] 0                                            \n","__________________________________________________________________________________________________\n","input_2 (InputLayer)            [(None, 188, 40, 1)] 0                                            \n","__________________________________________________________________________________________________\n","sequential (Sequential)         (None, 516)          23852804    input_1[0][0]                    \n","                                                                 input_2[0][0]                    \n","__________________________________________________________________________________________________\n","lambda (Lambda)                 (None, 516)          0           sequential[1][0]                 \n","                                                                 sequential[2][0]                 \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 1)            517         lambda[0][0]                     \n","==================================================================================================\n","Total params: 23,853,321\n","Trainable params: 23,853,065\n","Non-trainable params: 256\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KSD94RegN5yz","colab_type":"text"},"source":["### Load Best Weights and Compile"]},{"cell_type":"code","metadata":{"id":"Z6Anume5sDYc","colab_type":"code","colab":{}},"source":["model.load_weights(os.path.join(model_path, \"seg_weights.best.hdf5\"))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bpKNehMjsDYl","colab_type":"code","colab":{}},"source":["optimizer = Adam(lr = 0.00006)\n","model.compile(loss=\"binary_crossentropy\",metrics=['accuracy'], optimizer=optimizer)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2YXShuvrMM-5","colab_type":"text"},"source":["# Predictions"]},{"cell_type":"code","metadata":{"id":"A0uYRJABsDY3","colab_type":"code","colab":{}},"source":["def predict_speaker(inputaudiofile):\n","    \n","    ### extract audio mfccs for the inputaudiofile.\n","    audio_file_path = os.path.join(input_file_path, inputaudiofile)\n","    mfcc = convert_mfcc(audio_file_path)\n","        \n","    input_l = []\n","    input_r = []\n","    s,w,h = X.shape\n","    input_l.append(mfcc.reshape(w,h,1))\n","    y_pred = np.zeros(shape=(s))\n","    \n","    ###loop through the audio mfccs of the baseline audio files.\n","     \n","    for  i in range(X.shape[0]):\n","        w,h = mfcc.shape\n","        pairs=[np.zeros((1 , w, h,1)) for i in range(2)]\n","        pairs[0][0,:,:,:] = input_l[0]\n","        pairs[1][0,:,:,:] = X[i].reshape(w,h,1)\n","        y_pred[i] = (model.predict(pairs).ravel())[0]\n","        \n","    return y_pred\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FsPfKniEsDZD","colab_type":"code","colab":{}},"source":["y_pred = predict_speaker('MJEB1_SI837.WAV.wav')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LXAbEmxvsDZH","colab_type":"code","outputId":"87272c98-ba1e-4c29-cafb-3724efbd7963","colab":{}},"source":["prediction = y[np.argmax(y_pred)]\n","print(prediction)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['MJEB1']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hzNPcK1zsDZN","colab_type":"code","outputId":"6c23795c-071e-4de6-9137-7a698a219b32","colab":{}},"source":["y_pred = predict_speaker('MRC0_SX438.WAV.wav')\n","prediction = y[np.argmax(y_pred)]\n","print(prediction)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['MRCG0']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ex4aQ1VrsDZT","colab_type":"code","outputId":"8a1c588c-42e8-4fd0-9908-718eeb081933","colab":{}},"source":["y_pred = predict_speaker('sx52-Sindhu.wav')\n","prediction = y[np.argmax(y_pred)]\n","print(prediction)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['Sindhu']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NLT55BbDsDZX","colab_type":"code","outputId":"5ab46407-27cb-49e1-f95c-88038d4f86d0","colab":{}},"source":["y_pred = predict_speaker('sx30.wav')\n","prediction = y[np.argmax(y_pred)]\n","print(prediction)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['Joy']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WUgMgNbZsDZb","colab_type":"code","outputId":"ce00fb8e-a2e5-4889-b149-09ad9627072b","colab":{}},"source":["y_pred = predict_speaker('Sek10.wav')\n","#print(y_pred)\n","prediction = y[np.argmax(y_pred)]\n","print(prediction)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['Sekhar']\n"],"name":"stdout"}]}]}